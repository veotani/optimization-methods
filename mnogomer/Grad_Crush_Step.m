function [answer, iter] = Grad_Crush_Step(x0)
%градиентный метод с дроблением шага
alpha = 0.03; eps=0.08; la = 0.8; delta = 0.01;
%f = @(x) x(1)^2 + 2*x(2)^2 + x(1)^2*x(2)^2 + 2*x(3) + x(2)^2 + x(3)^2 - x(2);
f = @(x) x(1)^2 + 2*x(2)^2 + x(1)^2*x(2)^2 + 2*x(3) + exp(x(2)^2 + x(3)^2) - x(2);
f1 = @(x) [ 2*x(1) + 2*x(1)*x(2)^2;
            4*x(2) + 2*x(1)^2*x(2) + 2*x(2)*exp(x(2)^2 + x(3)^2) - 1;
            2 + 2*x(3)*exp(x(2)^2 + x(3)^2)];
criteria = 0;
iter = 0;
while (criteria == 0)
    x = x0 - alpha.*f1(x0);
    fun = f1(x0);
    iter = iter+1;
    if (f(x) - f(x0) > -alpha*eps*(fun(1)^2+fun(2)^2+fun(3)^2)) 
        alpha = alpha/la;
    else
        x0 = x0 - alpha.*f1(x0);
        fun = f1(x0);
        if ((fun(1)^2+fun(2)^2+fun(3)^2)<delta)
            answer = x0;
            criteria = 1;
        end
    end
end
%ВЫВОД: во время отладки алгоритма я сразу заметил большую зависимость от
%параметров lamda, eps, alpha, т.к. алгоритм достигал положительного
%результата только в случаях, когда эти числа подбирались удобно для его
%выполнения. алгоритму уже нужно больше итераций, но
%зато нам не приходится обращаться к одномерной оптимизации, что в конечном
%итоге даёт более оптимальный результат.
end

